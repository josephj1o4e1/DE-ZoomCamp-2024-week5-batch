{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 23:54:13 WARN Utils: Your hostname, Acer-JC resolves to a loopback address: 127.0.1.1; using 172.26.36.16 instead (on interface eth0)\n",
      "24/03/04 23:54:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 23:54:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 23:54:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('hw5') \\\n",
    "    .getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz to data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv.gz\n",
      "--2024-03-04 18:19:23--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.82.112.3, 205.251.194.8, 205.251.193.165, ...\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240304T231926Z&X-Amz-Expires=300&X-Amz-Signature=e8e1a5429a9731653333900a4ab3e7861ded55e30d08d957b23f062217f2368f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-04 18:19:23--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240304T231926Z&X-Amz-Expires=300&X-Amz-Signature=e8e1a5429a9731653333900a4ab3e7861ded55e30d08d957b23f062217f2368f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv.gz’\n",
      "\n",
      "data/raw/fhv/2019/1 100%[===================>]  18.48M  6.29MB/s    in 2.9s    \n",
      "\n",
      "2024-03-04 18:19:27 (6.29 MB/s) - ‘data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sh download_data_hw.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = f'data/raw/fhv/2019/10/'\n",
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(input_path)\n",
    "\n",
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_schema = types.StructType([\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PUlocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOlocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.IntegerType(), True),\n",
    "    types.StructField(\"Affiliated_base_number\", types.DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2019/10 fhv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'processing data for 2019/10 fhv')\n",
    "\n",
    "input_path = f'data/raw/fhv/2019/10/'\n",
    "output_path = f'data/pq/fhv/2019/10/'\n",
    "\n",
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhv_schema) \\\n",
    "    .csv(input_path)\n",
    "\n",
    "df_fhv \\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36M\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1    0 Mar  4 18:38 _SUCCESS\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00000-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00001-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00002-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00003-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00004-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 josephj1o4e1 josephj1o4e1 6.0M Mar  4 18:38 part-00005-6ea2c22e-8ee6-4c6d-b1d2-fdfb19df4d95-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pq/fhv/2019/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00350|2019-10-03 16:09:39|2019-10-03 16:18:27|         264|         173|   null|                  null|\n",
      "|              B00256|2019-10-19 20:12:27|2019-10-19 21:14:38|         264|         264|   null|                  null|\n",
      "|              B00256|2019-10-19 20:09:58|2019-10-19 21:21:21|         264|         264|   null|                  null|\n",
      "|              B00256|2019-10-17 07:35:15|2019-10-17 07:39:06|         264|         264|   null|                  null|\n",
      "|              B02095|2019-10-16 10:36:48|2019-10-16 10:43:12|         264|         264|   null|                  null|\n",
      "|              B02550|2019-10-04 10:34:08|2019-10-04 10:57:07|         264|          29|   null|                  null|\n",
      "|              B02715|2019-10-27 23:32:23|2019-10-27 23:58:58|           1|         164|   null|                  null|\n",
      "|              B02111|2019-10-03 07:48:17|2019-10-03 08:11:21|         258|          92|   null|                  null|\n",
      "|              B02972|2019-10-02 08:26:54|2019-10-02 08:43:29|         264|         244|   null|                  null|\n",
      "|              B01145|2019-10-05 22:36:37|2019-10-05 23:01:49|         264|         212|   null|                  null|\n",
      "|              B02868|2019-10-14 11:30:00|2019-10-14 11:50:00|         264|         264|   null|                  null|\n",
      "|              B01452|2019-10-28 14:38:24|2019-11-03 15:15:37|          84|         118|   null|                  null|\n",
      "|              B01061|2019-10-19 09:14:36|2019-10-19 09:18:24|         264|         126|   null|                  null|\n",
      "|              B02881|2019-10-29 12:08:10|2019-10-29 13:09:32|          82|         263|   null|                  null|\n",
      "|              B02546|2019-10-18 21:21:25|2019-10-18 21:47:54|         264|         235|   null|                  null|\n",
      "|              B02905|2019-10-05 11:40:21|2019-10-05 11:49:36|         264|         228|   null|                  null|\n",
      "|              B00821|2019-10-01 01:01:07|2019-10-01 01:06:20|         264|          63|   null|                  null|\n",
      "|              B00856|2019-10-18 15:10:18|2019-10-18 15:24:28|         264|          76|   null|                  null|\n",
      "|              B03162|2019-10-11 10:53:32|2019-10-11 11:13:58|          39|         188|   null|                  null|\n",
      "|              B03061|2019-10-24 10:15:25|2019-10-24 10:28:24|          78|          60|   null|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv_pq = spark.read.parquet('data/pq/fhv/*/*')\n",
    "df_fhv_pq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_pq.createOrReplaceTempView('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many taxi trips were there on the 15th of October?\n",
    "# Consider only trips that started on the 15th of October.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT count(*)\n",
    "FROM fhv\n",
    "where pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\n",
    ";\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|          time_spent|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|INTERVAL '26298 0...|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|INTERVAL '26298 0...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+--------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|          time_spent|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|INTERVAL '26298 0...|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|INTERVAL '26298 0...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+--------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|          time_spent|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|INTERVAL '26298 0...|\n",
      "+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the length of the longest trip in the dataset in hours?\n",
    "# Just playing, I want to select the rows with max time spent along with the pickup/dropoff datetime. \n",
    "\n",
    "# Subquery + MAX()\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT pickup_datetime, dropoff_datetime, (dropoff_datetime - pickup_datetime) as time_spent\n",
    "FROM fhv\n",
    "WHERE (dropoff_datetime - pickup_datetime) = (SELECT MAX(dropoff_datetime - pickup_datetime) from fhv)\n",
    ";\n",
    "\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT f1.pickup_datetime, f1.dropoff_datetime, (f1.dropoff_datetime - f1.pickup_datetime) as time_spent\n",
    "FROM fhv f1, (select MAX(dropoff_datetime - pickup_datetime) as max_time_spent from fhv) f2\n",
    "WHERE (f1.dropoff_datetime - f1.pickup_datetime) = f2.max_time_spent\n",
    ";\n",
    "\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "# ORDER BY + LIMIT...can't deal with \"several\" max rows\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT pickup_datetime, dropoff_datetime, (dropoff_datetime - pickup_datetime) as time_spent\n",
    "FROM fhv\n",
    "ORDER BY time_spent DESC\n",
    "LIMIT 1\n",
    ";\n",
    "\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read \\\n",
    "    # .option(\"header\", \"true\") \\\n",
    "    # .csv('taxi+_zone_lookup.csv')\n",
    "# df.write.parquet('zones')\n",
    "\n",
    "df_zones_pq = spark.read.parquet('tmp/zones/*')\n",
    "df_zones_pq.show()\n",
    "df_zones_pq.createOrReplaceTempView('zones')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:============================>                           (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       Zone|\n",
      "+-----------+\n",
      "|Jamaica Bay|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Least frequent pickup location zone\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT PULocationID, count(*) as freq\n",
    "FROM fhv\n",
    "GROUP BY PULocationID\n",
    "ORDER BY freq ASC\n",
    ";\n",
    "\n",
    "\"\"\").createOrReplaceTempView('PU_freq')\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT z.Zone\n",
    "FROM (SELECT PULocationID\n",
    "    FROM PU_freq\n",
    "    WHERE PU_freq.freq = (\n",
    "        SELECT MIN(freq)\n",
    "        FROM PU_freq\n",
    "    )) q1 \n",
    "    JOIN zones z ON (q1.PULocationID=z.LocationID)\n",
    ";\n",
    "\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
